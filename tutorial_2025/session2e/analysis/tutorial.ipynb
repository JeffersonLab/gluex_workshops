{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Aggregate and Plot AmpTools .fit Results\n",
    "You now have a whole bunch of fit results, stored as `.fit` files, that you need to start plotting. Within this tutorial you will learn how to:\n",
    "1. Aggregate these fit and data files into flattened `.csv` files\n",
    "2. Load and plot the `.csv` fit results using python's pandas and matplotlib libraries   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "1. At the top right of the notebook your language / kernel is listed. Make sure `.venv (Python 3.9.18)` is selected. If the option does not appear, make sure to the virtual environment is active (see [**section 1.2** in the README](../README.md) for details).\n",
    "    * If you can't find it, you may need to run the command `python -m ipykernel install --prefix=/path/to/alternative/location --name=.venv --display-name \"Python (.venv)\"`\n",
    "2. Next we want to ensure that our GlueX environment is setup. Normally we could simply run `source setup_gluex.csh` if we were doing this in the terminal, but we'll need to go about it a special way to run this in the jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# first lets define the parent dir (this session)\n",
    "PARENT_DIR = Path.cwd().resolve().parent\n",
    "print(PARENT_DIR)\n",
    "\n",
    "# We'll also need to set the data directory\n",
    "DATA_DIR = \"/work/halld/gluex_workshop_data/tutorial_2025/session2e\"\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, PARENT_DIR) # add the session directory to the list of directories Python uses to look for modules\n",
    "\n",
    "# run the source script (done here in csh, but bash could be done instead)\n",
    "command = f\"source {PARENT_DIR}/../setup_gluex.csh && env\"\n",
    "proc = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, executable='/bin/csh')\n",
    "output, _ = proc.communicate()\n",
    "\n",
    "# Parse the environment variables\n",
    "env_vars = {}\n",
    "for line in output.decode().splitlines():\n",
    "    key, value = line.split('=', 1)\n",
    "    env_vars[key] = value\n",
    "\n",
    "# add the environment variables\n",
    "os.environ.update(env_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Aggregation\n",
    "We will want to create the following 2 files in preparation for our analysis:\n",
    "1. `data.csv`: constructed from the `.root` data files in each mass bin, containing the information for that bin\n",
    "2. `best_fits.csv`: contains all the fit results across the entire mass range, made from the \"best\" of all the randomized fits in each bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "If we want to plot our fit results, we need to include the original data we are actually fitting to. This information is unfortunately not included in the `.fit` files, so we need to read it into a `data.csv` file using [convert_to_csv.py](../scripts/convert_to_csv.py). These python scripts use `argparse`, so we can conveniently see its abilities through its help message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i $PARENT_DIR/scripts/convert_to_csv.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now lets see what (sorted) files we are going to combine, and run the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i $PARENT_DIR/scripts/convert_to_csv.py -i $DATA_DIR/mass*/*Amplitude.root -p\n",
    "%run -i $PARENT_DIR/scripts/convert_to_csv.py -i $DATA_DIR/mass*/*Amplitude.root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see a `data.csv` file here in the [analysis directory](./)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Results\n",
    "Now we will be combining all our `.fit` results across the mass bins into a flattened `.csv` file to prepare them for analysis via python. This is achieved again through the [convert_to_csv.py](../scripts/convert_to_csv.py) script, which behind the scenes interacts with [extract_fit_results.cc](../scripts/extract_fit_results.cc) to load the AmpTools `FitResults` class and import the information we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i $PARENT_DIR/scripts/convert_to_csv.py -i $DATA_DIR/mass*/omegapi.fit -o best_fits.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alongside the fit result `best_fits.csv` you'll also find 2 other csvs which contain the full covariance and correlation matrix of each fit result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "To ensure that our data is ready for analysis, lets load in our files using pandas dataframes and check them for any potential issues. First, we can simply print out the heads (first 5 rows) to get a sense of the structure. Note if you are working on VS Code and installed the suggested [DataWrangler extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.datawrangler), then you can open these dataframes directly from this notebook for easy viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load files\n",
    "df_fit = pd.read_csv(\"best_fits.csv\")\n",
    "df_data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "df_fit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check that we have the same number of data files and fit files, and for any missing / NaN values:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_fit.shape[0] == df_data.shape[0])\n",
    "print(f\"Number of null values in the fit results: {df_fit.isnull().sum().sum()}\")\n",
    "print(f\"Number of null values in the data: {df_data.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we'll want to wrap our phase differences to all be within a $2\\pi$ range, and converted from radians to degrees $(^\\circ)$. Lets wrap them within $(-\\pi,\\pi]$ using our helper functions in [utils](./utils.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "utils.wrap_phases(df_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "With our data prepared, lets move on to analysis. To avoid having to edit these settings for every single figure, lets change the global defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams.update({\n",
    "    'figure.figsize': (10, 8),\n",
    "    'figure.dpi': 100,\n",
    "    'xtick.labelsize': 14,\n",
    "    'ytick.labelsize': 14,\n",
    "    'axes.labelsize': 16,\n",
    "    'legend.fontsize': 16,\n",
    "    'xtick.major.width': 2.0,\n",
    "    'ytick.major.width': 2.0,\n",
    "    'xtick.minor.width': 1.8,\n",
    "    'ytick.minor.width': 1.8,\n",
    "    'lines.markersize': 10,\n",
    "    'grid.alpha': 0.8,\n",
    "    'axes.grid': True\n",
    "})\n",
    "plt.minorticks_on()\n",
    "plt.close() # suppress outputting an empty plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also define a few common parameters to all plots, and print out what coherent sums and phase differences we have available to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_bins = df_data[\"m_center\"]\n",
    "bin_width = (df_data[\"m_high\"] - df_data[\"m_low\"])[0] # all bin widths are equal, so just use the first one\n",
    "coherent_sums = utils.get_coherent_sums(df_fit)\n",
    "phase_differences = utils.get_phase_differences(df_fit)\n",
    "\n",
    "for sum_type, sum_list in coherent_sums.items():\n",
    "    print(f\"{sum_type} -> {sum_list}\")\n",
    "for amp_pair, phase_column in phase_differences.items():\n",
    "    print(f\"{amp_pair} -> {phase_column}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mass independent fit results\n",
    "One of the simplest plots to make is of the data we fit to with the fit result's intensity in bins of mass, with the fit result decomposed into the coherent sum of our $J^P$ values. In this case we only have a $b_1$ contribution, and so only $1^+$ contributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I prefer this colormap over the default\n",
    "jp_colors = matplotlib.colormaps[\"Dark2\"].colors\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# plot data as black dots\n",
    "ax.errorbar(\n",
    "    x=mass_bins, y=df_data[\"events\"], xerr=bin_width / 2.0 , yerr=df_data[\"events_err\"], \n",
    "    fmt=\"k.\",label=\"Signal MC\",\n",
    ")\n",
    "\n",
    "# Plot Fit Result as a grey histogram with its error bars\n",
    "ax.bar(\n",
    "    x=mass_bins, height=df_fit[\"detected_events\"], width=bin_width,        \n",
    "    color=\"0.1\", alpha=0.15, label=\"Fit Result\",\n",
    ")\n",
    "ax.errorbar(\n",
    "    x=mass_bins, y=df_fit[\"detected_events\"], yerr=df_fit[\"detected_events_err\"],\n",
    "    fmt=\",\", color=\"0.1\", alpha=0.2, markersize=0,\n",
    ")\n",
    "\n",
    "# plot 1+\n",
    "ax.errorbar(\n",
    "    x=mass_bins, y=df_fit[\"1p\"], xerr=bin_width/2.0, yerr=df_fit[\"1p_err\"], \n",
    "    label=utils.convert_amp_name(\"1p\"), # converts amplitude / phase differences to be in J^P L_m^(e) format\n",
    "    linestyle=\"\", # want only markers, no lines\\\n",
    "    marker=\"1\",\n",
    "    color=jp_colors[2]\n",
    ")\n",
    "\n",
    "# plot 1-\n",
    "ax.errorbar(\n",
    "    x=mass_bins, y=df_fit[\"1m\"], xerr=bin_width/2.0, yerr=df_fit[\"1m_err\"], \n",
    "    label=utils.convert_amp_name(\"1m\"), # converts amplitude / phase differences to be in J^P L_m^(e) format\n",
    "    linestyle=\"\", # want only markers, no lines\\\n",
    "    marker=\"s\",\n",
    "    color=jp_colors[3]\n",
    ")\n",
    "\n",
    "ax.set_xlabel(r\"$\\omega\\pi^0$ inv. mass $(GeV)$\", loc=\"right\")\n",
    "ax.set_ylabel(f\"Events / {bin_width:.3f} GeV\", loc=\"top\")\n",
    "ax.set_ylim(bottom=0.0)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the individual waves perform though? Lets view them in a grid of $m$-projections and $L$ angular momenta values, where each plot shares the reflectivity contributions. The reflectivities will be colored according to the unofficial official convention for $\\color{red}natural~(\\varepsilon=+1)$ and $\\color{blue}unnatural~(\\varepsilon=-1)$ exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some dictionaries to convert characters <-> integers\n",
    "char_to_int = {\"m\": -1, \"0\": 0, \"p\": +1, \"S\": 0, \"P\": 1, \"D\": 2}\n",
    "int_to_char = {-1: \"m\", 0: \"0\", +1: \"p\"}\n",
    "pm_dict = {\"m\": \"-\", \"p\": \"+\"}\n",
    "\n",
    "# sort the JPL values by the order of S, P, D, F waves, and the m-projections\n",
    "jpl_values = sorted(coherent_sums[\"JPL\"], key=lambda JPL: char_to_int[JPL[-1]])\n",
    "m_ints = sorted({char_to_int[JPmL[-2]] for JPmL in coherent_sums[\"JPmL\"]})\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    nrows=len(jpl_values),\n",
    "    ncols=len(m_ints),\n",
    "    sharex=True,    \n",
    "    #sharey=True, # uncomment to compare relative intensities\n",
    "    figsize=(15, 10),\n",
    ")\n",
    "\n",
    "# iterate through JPL (sorted like S, P, D, F wave) and sorted m-projections\n",
    "for row, jpl in enumerate(jpl_values):\n",
    "    for col, m in enumerate(m_ints):\n",
    "\n",
    "        # force sci notation so large ticklabels don't overlap with neighboring plots\n",
    "        axs[row,col].ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0,0))\n",
    "        \n",
    "        # recombine jpl and m to get string needed to access the column\n",
    "        JPmL = f\"{jpl[0:2]}{int_to_char[m]}{jpl[-1]}\"\n",
    "\n",
    "        # set the labels for the first rows and columns\n",
    "        if row == 0:\n",
    "            axs[row, col].set_title(f\"m={m}\", fontsize=18)\n",
    "        if col == 0:\n",
    "            J = JPmL[0]\n",
    "            P = pm_dict[JPmL[1]]\n",
    "            L = JPmL[-1]\n",
    "            axs[row, col].set_ylabel(rf\"${J}^{{{P}}}{L}$\", fontsize=18)\n",
    "\n",
    "        # plot the negative reflectivity contribution        \n",
    "        neg_plot = axs[row, col].errorbar(\n",
    "            x=mass_bins, y=df_fit[f\"m{JPmL}\"], xerr=bin_width/2.0, yerr=df_fit[f\"m{JPmL}_err\"],\n",
    "            marker=\"v\", linestyle=\"\", markersize=8,\n",
    "            color=\"blue\", \n",
    "            alpha=0.5, # prevent overlap from cluttering the view\n",
    "            label=r\"$\\varepsilon=-1$\",\n",
    "        )\n",
    "        # plot the positive reflectivity contribution\n",
    "        pos_plot = axs[row, col].errorbar(\n",
    "            x=mass_bins, y=df_fit[f\"p{JPmL}\"], xerr=bin_width/2.0, yerr=df_fit[f\"p{JPmL}_err\"],\n",
    "            marker=\"^\", linestyle=\"\", markersize=8,\n",
    "            color=\"red\",\n",
    "            alpha=0.5,\n",
    "            label=r\"$\\varepsilon=+1$\",\n",
    "        )\n",
    "\n",
    "# reset limits to 0 for all plots\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.set_ylim(bottom=0)\n",
    "\n",
    "# figure cosmetics\n",
    "fig.text(0.5, 0.04, r\"$\\omega\\pi^0$ inv. mass (GeV)\", ha=\"center\", fontsize=20)\n",
    "fig.text(\n",
    "    0.04, 0.5, f\"Events / {bin_width:.3f} GeV\", \n",
    "    ha=\"center\", va=\"center\", \n",
    "    rotation=\"vertical\", rotation_mode=\"anchor\", fontsize=20,\n",
    ")\n",
    "\n",
    "# the pos/neg_plot variables get overwritten in the loop, so we're only passing one set of handles to the figure,\n",
    "# which is okay since all plots have the same legend. If we didn't do this, every single plots redundant legend would be displayed\n",
    "fig.legend(handles=[pos_plot, neg_plot], loc=\"upper right\") # \n",
    "plt.show()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the phase differences? Lets take a look at our two dominant amplitudes from separate $J^P$ values and plot their corresponding phase difference together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, sharex=True, gridspec_kw={\"wspace\": 0.0, \"hspace\": 0.07}, height_ratios=[3, 1])\n",
    "\n",
    "amp1, amp2 = \"p1p0S\", \"p1mpP\"\n",
    "\n",
    "# plot the first amplitude\n",
    "axs[0].errorbar(\n",
    "    x=mass_bins, y=df_fit[amp1], xerr=bin_width/2.0, yerr=df_fit[f\"{amp1}_err\"],\n",
    "    marker=\"o\", linestyle=\"\", color=\"green\",\n",
    "    label=utils.convert_amp_name(amp1),\n",
    ")\n",
    "# plot the second amplitude\n",
    "axs[0].errorbar(\n",
    "    x=mass_bins, y=df_fit[amp2], xerr=bin_width/2.0, yerr=df_fit[f\"{amp2}_err\"],\n",
    "    marker=\"s\", linestyle=\"\", color=\"purple\",\n",
    "    label=utils.convert_amp_name(amp2),\n",
    ")\n",
    "\n",
    "# plot the phase difference. Since there is an inherent ambiguity in the sign of the phase difference within\n",
    "# the model, we need to plot both signs to accommodate for every possible phase motion\n",
    "phase_dif = phase_differences[(amp1, amp2)]\n",
    "axs[1].errorbar(\n",
    "    x=mass_bins, y=df_fit[phase_dif], xerr=bin_width/2.0, yerr=df_fit[f\"{phase_dif}_err\"].abs(), \n",
    "    marker=\".\", linestyle=\"\", color=\"black\"\n",
    ")\n",
    "axs[1].errorbar(\n",
    "    x=mass_bins, y=-df_fit[phase_dif], xerr=bin_width/2.0, yerr=df_fit[f\"{phase_dif}_err\"].abs(), \n",
    "    marker=\".\", linestyle=\"\", color=\"black\"\n",
    ")\n",
    "\n",
    "# cosmetics\n",
    "axs[0].set_ylim(bottom=0.0)\n",
    "axs[0].set_ylabel(f\"Events / {bin_width:.3f} GeV\", loc=\"top\")\n",
    "\n",
    "axs[1].set_yticks(np.linspace(-180, 180, 5))  # force to be in pi intervals\n",
    "axs[1].set_ylim([-180, 180])\n",
    "axs[1].set_ylabel(r\"Phase Diff. ($^{\\circ}$)\", loc=\"center\")\n",
    "axs[1].set_xlabel(r\"$\\omega\\pi^0$ inv. mass $(GeV)$\", loc=\"right\")\n",
    "\n",
    "axs[0].legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix\n",
    "To see how correlated our amplitude real/imaginary parts are, lets load in the matrix for each file and view the matrix as a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df_corr = pd.read_csv(\"best_fits_corr.csv\")\n",
    "files = df_corr[\"file\"].unique()\n",
    "\n",
    "def convert_parameter_name(full_parameter):\n",
    "    \"\"\"\n",
    "    Convert the parameter name to a more readable format. Assumed to be in\n",
    "    reaction::sum::JPml_re/im format (where re OR im can be used)\n",
    "    \"\"\"\n",
    "\n",
    "    # drop the reaction name\n",
    "    sum = full_parameter.split(\"::\")[1]\n",
    "    parameter = full_parameter.split(\"::\")[-1].split(\"_\")[0] # assumed to be in jpml format\n",
    "    re_im_flag = full_parameter.split(\"::\")[-1].split(\"_\")[1] \n",
    "\n",
    "    # convert to eJPmL format\n",
    "    e = \"p\" if \"ImagPosSign\" or \"RealNegSign\" in sum else \"m\"\n",
    "    parameter = f\"{e}{parameter[:-1]}{parameter[-1].upper()}\"    \n",
    "\n",
    "    # convert to J^P L_m^(e) format\n",
    "    parameter = utils.convert_amp_name(parameter)\n",
    "    \n",
    "    if re_im_flag == \"re\":\n",
    "        parameter = f\"Re({parameter})\"\n",
    "    elif re_im_flag == \"im\":\n",
    "        parameter = f\"Im({parameter})\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown re/im flag: {re_im_flag}\")\n",
    "\n",
    "    return parameter\n",
    "\n",
    "for file in files:    \n",
    "    df_corr_file = df_corr[df_corr[\"file\"] == file].copy()\n",
    "    df_corr_file = df_corr_file.drop(columns=[\"file\"]) # remove the file column\n",
    "    df_corr_file = df_corr_file.loc[~df_corr_file[\"parameter\"].str.contains(\"Bkgd\", na=False)]\n",
    "    df_corr_file = df_corr_file.drop(columns=[col for col in df_corr_file.columns if \"Bkgd\" in col])\n",
    "\n",
    "    # remove columns whose correlations are all zero\n",
    "    df_corr_file = df_corr_file.loc[:, (df_corr_file != 0).any(axis=0)]\n",
    "\n",
    "    # Convert parameter headers and the parameter column\n",
    "    df_corr_file = df_corr_file.rename(columns=lambda col: convert_parameter_name(col) if col != \"parameter\" else col)\n",
    "    df_corr_file[\"parameter\"] = df_corr_file[\"parameter\"].apply(convert_parameter_name)\n",
    "\n",
    "    df_corr_file = df_corr_file.set_index(\"parameter\")\n",
    "\n",
    "    df = df.loc[~df[\"parameter\"].str.contains(\"1m\", na=False)]\n",
    "    df = df.drop(columns=[c for c in df.columns if \"1m\" in c])\n",
    "    \n",
    "    plt.figure(figsize=(15, 15))\n",
    "    sns.heatmap(df_corr_file.corr(), annot=True, fmt=\".1f\", cmap=\"coolwarm\", cbar=True)\n",
    "    plt.title(f\"Correlation Matrix - {file}\")    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
